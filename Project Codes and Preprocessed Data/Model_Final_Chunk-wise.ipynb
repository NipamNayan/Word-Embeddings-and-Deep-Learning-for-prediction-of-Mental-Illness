{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "#from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"C://Users//SONY//Desktop//WORK//PROJECT//train_chunks.xlsx\"\n",
    "test = \"C://Users//SONY//Desktop//WORK//PROJECT//test_chunks.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_excel(train)\n",
    "df=df.dropna(axis=0)\n",
    "df.Text = df.Text.str.replace('[^\\w\\s]','')\n",
    "df.Text = df.Text.str.lower()\n",
    "#df.Text = df.Text.str.split()\n",
    "\n",
    "df1 = pd.read_excel(test)\n",
    "df1=df1.dropna(axis=0)\n",
    "df1.Text = df1.Text.str.replace('[^\\w\\s]','')\n",
    "df1.Text = df1.Text.str.lower()\n",
    "#df1.Text = df1.Text.str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TACKLING WITH UNBALANCED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['Labels']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A= df[df['Labels']==1]\n",
    "B= df[df['Labels']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_repeated = pd.concat([A]*7, ignore_index=True)\n",
    "df = pd.concat([A_repeated,B])\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ya itd be bs if it wasnt balanced like tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject2419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>thanks everyone for the kind words33 i had...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject6681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>dat hair roto     your sister might like t...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject3763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>in what context       i have been getting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject6446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>lol      simple mistake call them      tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject3788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2714</td>\n",
       "      <td>do you sometimes not cross the ts as well</td>\n",
       "      <td>1</td>\n",
       "      <td>subject3883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2715</td>\n",
       "      <td>you can eat as much as you wish in front o...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject2840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2716</td>\n",
       "      <td>if it wasnt pink i would cop this nice fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject3498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2717</td>\n",
       "      <td>adorable little derpface d i love him hes ...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject1913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2718</td>\n",
       "      <td>this is the final stage of capitalism the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject3274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2719 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Labels           Id\n",
       "0         ya itd be bs if it wasnt balanced like tha...       0  subject2419\n",
       "1         thanks everyone for the kind words33 i had...       1  subject6681\n",
       "2         dat hair roto     your sister might like t...       0  subject3763\n",
       "3         in what context       i have been getting ...       1  subject6446\n",
       "4         lol      simple mistake call them      tha...       0  subject3788\n",
       "...                                                 ...     ...          ...\n",
       "2714         do you sometimes not cross the ts as well        1  subject3883\n",
       "2715      you can eat as much as you wish in front o...       0  subject2840\n",
       "2716      if it wasnt pink i would cop this nice fin...       0  subject3498\n",
       "2717      adorable little derpface d i love him hes ...       1  subject1913\n",
       "2718      this is the final stage of capitalism the ...       0  subject3274\n",
       "\n",
       "[2719 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FURTHER PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords \n",
    "#from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop = stopwords.words('english')\n",
    "#df['Text'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df.Text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1['Text'] = df1['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df1['Text'] = df1.Text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING GLOVE VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer:\n",
    "    def __init__(self):\n",
    "        # load in pre-trained word vectors\n",
    "        print('Loading word vectors...')\n",
    "        word2vec = {}\n",
    "        embedding = []\n",
    "        idx2word = []\n",
    "        with open('C:\\\\Users\\\\SONY\\\\Desktop\\\\WORK\\\\New\\\\glove\\\\glove.6B.100d.txt',encoding='utf') as f:\n",
    "            # is just a space-separated text file in the format:\n",
    "            # word vec[0] vec[1] vec[2] ...\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vec = np.asarray(values[1:], dtype='float32')\n",
    "                word2vec[word] = vec\n",
    "                embedding.append(vec)\n",
    "                idx2word.append(word)\n",
    "        print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "        # save for later\n",
    "        self.word2vec = word2vec\n",
    "        self.embedding = np.array(embedding)\n",
    "        self.word2idx = {v:k for k,v in enumerate(idx2word)}\n",
    "        self.V, self.D = self.embedding.shape\n",
    "\n",
    "    def fit(self, data):\n",
    "        pass\n",
    "\n",
    "    def transform(self, data):\n",
    "        X = np.zeros((len(data), self.D))\n",
    "        Y = np.zeros((len(data), self.D))\n",
    "        Z = np.zeros((len(data), self.D))\n",
    "        n = 0\n",
    "        emptycount = 0\n",
    "        for sentence in data:\n",
    "            #tokens = sentence.lower().split()\n",
    "            vecs = []\n",
    "            for word in sentence:\n",
    "                if word in self.word2vec:\n",
    "                    vec = self.word2vec[word]\n",
    "                    vecs.append(vec)\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X[n] = vecs.mean(axis=0)\n",
    "                Y[n] = vecs.min(axis=0)\n",
    "                Z[n] = vecs.max(axis=0)\n",
    "                \n",
    "            else:\n",
    "                emptycount += 1\n",
    "            n += 1\n",
    "        print(\"Number of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "        return X,Y,Z\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n",
      "Number of samples with no words found: 0 / 2719\n",
      "Number of samples with no words found: 2 / 3198\n"
     ]
    }
   ],
   "source": [
    "vectorizer = GloveVectorizer()\n",
    "# vectorizer = Word2VecVectorizer()\n",
    "Xavg,Xmin,Xmax = vectorizer.fit_transform(df.Text)\n",
    "Ytrain = df.Labels\n",
    "Test_avg,Test_min,Test_max = vectorizer.transform(df1.Text)\n",
    "Ytest = df1.Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2719, 100), array([[-0.103516  ,  0.074485  ,  0.22089323, ..., -0.17393164,\n",
       "          0.41592377, -0.05083651],\n",
       "        [-0.06503699,  0.20887189,  0.3932212 , ..., -0.26896009,\n",
       "          0.39756155,  0.21378291],\n",
       "        [-0.02464631,  0.19230519,  0.41903526, ..., -0.28894946,\n",
       "          0.48738915,  0.20868236],\n",
       "        ...,\n",
       "        [-0.14407238,  0.19350828,  0.43613529, ..., -0.32242751,\n",
       "          0.25936514,  0.2879793 ],\n",
       "        [-0.14042388,  0.17408687,  0.24441531, ..., -0.26558679,\n",
       "          0.44454801,  0.20874248],\n",
       "        [-0.1039901 ,  0.23156282,  0.32641757, ..., -0.20961121,\n",
       "          0.33832663,  0.24124797]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xavg.shape, Xavg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRYING SIMPLER ML MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='liblinear',class_weight='balanced',random_state=5,tol=0.001,max_iter=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8444280985656492\n",
      "test score: 0.8542839274546592\n"
     ]
    }
   ],
   "source": [
    "#model = RandomForestClassifier(n_estimators=200)\n",
    "model.fit(Xavg, Ytrain)\n",
    "print(\"train score:\", model.score(Xavg, Ytrain))\n",
    "print(\"test score:\", model.score(Test_avg, Ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(Test_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score,accuracy_score, classification_report, f1_score ,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2419  369]\n",
      " [  97  313]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91      2788\n",
      "           1       0.46      0.76      0.57       410\n",
      "\n",
      "    accuracy                           0.85      3198\n",
      "   macro avg       0.71      0.82      0.74      3198\n",
      "weighted avg       0.90      0.85      0.87      3198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(Ytest,predictions)\n",
    "print(cm)\n",
    "print('\\n',classification_report(Ytest,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2241  547]\n",
      " [ 158  252]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86      2788\n",
      "           1       0.32      0.61      0.42       410\n",
      "\n",
      "    accuracy                           0.78      3198\n",
      "   macro avg       0.62      0.71      0.64      3198\n",
      "weighted avg       0.85      0.78      0.81      3198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(Xmax, Ytrain)\n",
    "predictions2 = model.predict(Test_max)\n",
    "cm2 = confusion_matrix(Ytest,predictions2)\n",
    "print(cm2)\n",
    "print('\\n',classification_report(Ytest,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2213  575]\n",
      " [ 137  273]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86      2788\n",
      "           1       0.32      0.67      0.43       410\n",
      "\n",
      "    accuracy                           0.78      3198\n",
      "   macro avg       0.63      0.73      0.65      3198\n",
      "weighted avg       0.86      0.78      0.81      3198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(Xmin, Ytrain)\n",
    "predictions3 = model.predict(Test_min)\n",
    "cm3 = confusion_matrix(Ytest,predictions3)\n",
    "print(cm3)\n",
    "print('\\n',classification_report(Ytest,predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "SVMM = svm.LinearSVC(class_weight='balanced',verbose=0, random_state=None,max_iter=1000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8521515262964325\n",
      "test score: 0.8564727954971857\n",
      "\n",
      " [[2436  352]\n",
      " [ 107  303]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91      2788\n",
      "           1       0.46      0.74      0.57       410\n",
      "\n",
      "    accuracy                           0.86      3198\n",
      "   macro avg       0.71      0.81      0.74      3198\n",
      "weighted avg       0.89      0.86      0.87      3198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVMM.fit(Xavg, Ytrain)\n",
    "predictions4 = SVMM.predict(Test_avg)\n",
    "cm4 = confusion_matrix(Ytest,predictions4)\n",
    "print(\"train score:\", SVMM.score(Xavg, Ytrain))\n",
    "print(\"test score:\", SVMM.score(Test_avg, Ytest))\n",
    "print('\\n',cm4)\n",
    "print('\\n',classification_report(Ytest,predictions4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL MODEL FOR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#import sklearn\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.dstack([Xavg,Xmin,Xmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2719, 100, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(2719, 1,100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.dstack([Test_avg,Test_min,Test_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3198, 100, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.reshape(3198, 1,100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.asarray(pd.get_dummies(Ytrain), dtype = np.int8)\n",
    "test_y = np.asarray(pd.get_dummies(Ytest), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2719, 1, 100, 3), (2719, 2), (3198, 1, 100, 3), (3198, 2))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_pooling(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 100\n",
    "num_labels = 2\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 20\n",
    "kernel_size = 5\n",
    "nbr_kernels = 30\n",
    "num_hidden = 128\n",
    "\n",
    "learning_rate = 0.001\n",
    "momentum=0.9\n",
    "decay_rate = 0.0005\n",
    "epsilon = 0.01\n",
    "training_epochs = 30\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING THE LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_conv(X,kernel_size,num_channels,nbr_kernels)        #FILTER LAYER 1\n",
    "p = apply_pooling(c,10,2)                                     #POOLING LAYER 1\n",
    "\n",
    "shape = p.get_shape().as_list()\n",
    "p_flat = tf.reshape(p, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "W = weight_variable([shape[1] * shape[2] * nbr_kernels * num_channels, num_hidden])\n",
    "b = bias_variable([num_hidden])\n",
    "f = tf.nn.relu(tf.add(tf.matmul(p_flat, W),b))\n",
    "\n",
    "W2 = weight_variable([num_hidden, num_hidden])\n",
    "b2 = bias_variable([num_hidden])\n",
    "f2 = tf.nn.relu(tf.add(tf.matmul(f, W2),b2))\n",
    "\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "softmax_out = tf.nn.softmax(tf.matmul(f2, out_weights) + out_biases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(softmax_out))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay= decay_rate , momentum= momentum,epsilon= epsilon, use_locking=False, centered=False, name='RMSProp').minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(softmax_out,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  15.010111  Training Accuracy:  0.77013606\n",
      "Epoch:  1  Training Loss:  12.355821  Training Accuracy:  0.7513792\n",
      "Epoch:  2  Training Loss:  10.102001  Training Accuracy:  0.78116953\n",
      "Epoch:  3  Training Loss:  9.26745  Training Accuracy:  0.7907319\n",
      "Epoch:  4  Training Loss:  6.3943596  Training Accuracy:  0.77969843\n",
      "Epoch:  5  Training Loss:  6.6742225  Training Accuracy:  0.64950347\n",
      "Epoch:  6  Training Loss:  10.638524  Training Accuracy:  0.81243104\n",
      "Epoch:  7  Training Loss:  31.852676  Training Accuracy:  0.53438765\n",
      "Epoch:  8  Training Loss:  11.296098  Training Accuracy:  0.7833762\n",
      "Epoch:  9  Training Loss:  7.7995186  Training Accuracy:  0.6811328\n",
      "Epoch:  10  Training Loss:  6.490534  Training Accuracy:  0.7035675\n",
      "Epoch:  11  Training Loss:  9.471545  Training Accuracy:  0.62522984\n",
      "Epoch:  12  Training Loss:  4.741336  Training Accuracy:  0.8584038\n",
      "Epoch:  13  Training Loss:  3.6864538  Training Accuracy:  0.78926075\n",
      "Epoch:  14  Training Loss:  15.942212  Training Accuracy:  0.6171386\n",
      "Epoch:  15  Training Loss:  26.481653  Training Accuracy:  0.6472968\n",
      "Epoch:  16  Training Loss:  10.902609  Training Accuracy:  0.58403826\n",
      "Epoch:  17  Training Loss:  12.433722  Training Accuracy:  0.6594336\n",
      "Epoch:  18  Training Loss:  2.9457133  Training Accuracy:  0.77271056\n",
      "Epoch:  19  Training Loss:  4.242063  Training Accuracy:  0.73997796\n",
      "Epoch:  20  Training Loss:  25.688004  Training Accuracy:  0.8289812\n",
      "Epoch:  21  Training Loss:  5.3647523  Training Accuracy:  0.8837808\n",
      "Epoch:  22  Training Loss:  6.7677484  Training Accuracy:  0.7443913\n",
      "Epoch:  23  Training Loss:  17.165274  Training Accuracy:  0.8275101\n",
      "Epoch:  24  Training Loss:  2.7305617  Training Accuracy:  0.9058477\n",
      "Epoch:  25  Training Loss:  33.332718  Training Accuracy:  0.6598014\n",
      "Epoch:  26  Training Loss:  33.497932  Training Accuracy:  0.6539169\n",
      "Epoch:  27  Training Loss:  3.3023775  Training Accuracy:  0.8808386\n",
      "Epoch:  28  Training Loss:  22.901955  Training Accuracy:  0.5947039\n",
      "Epoch:  29  Training Loss:  2.386867  Training Accuracy:  0.88525194\n",
      "Testing Accuracy: 0.78236395\n"
     ]
    }
   ],
   "source": [
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    #summary_writer = tf.summary.FileWriter('./tensor/logs', graph=tf.get_default_graph())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            #_, c,summary = session.run([optimizer, loss,merged],feed_dict={X: batch_x, Y : batch_y})\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "            #summary_writer.add_summary(summary, epoch)\n",
    "        print(\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))\n",
    "    preds = session.run(softmax_out, feed_dict={X: test_x, Y: test_y})\n",
    "    #save_path = saver.save(session, \"/tmp/model.ckpt\")\n",
    "    #print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3198, 2)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.6843599e-03, 9.9431568e-01],\n",
       "       [2.4683070e-05, 9.9997532e-01],\n",
       "       [4.0279412e-01, 5.9720594e-01],\n",
       "       ...,\n",
       "       [6.5125544e-03, 9.9348748e-01],\n",
       "       [9.9660599e-01, 3.3940000e-03],\n",
       "       [2.8382918e-01, 7.1617085e-01]], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred=[]\n",
    "for i in preds:\n",
    "    final_pred.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[2183  605]\n",
      " [  91  319]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86      2788\n",
      "           1       0.35      0.78      0.48       410\n",
      "\n",
      "    accuracy                           0.78      3198\n",
      "   macro avg       0.65      0.78      0.67      3198\n",
      "weighted avg       0.88      0.78      0.81      3198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm5 = confusion_matrix(Ytest,final_pred)\n",
    "print('\\n',cm5)\n",
    "print('\\n',classification_report(Ytest,final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
